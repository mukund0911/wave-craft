# VoiceCraft Integration Guide

## üéØ Overview

WaveCraft now supports **voice cloning** for speech editing using VoiceCraft! This allows you to:
- ‚úÖ Modify transcribed text while preserving the original speaker's voice
- ‚úÖ Add/remove words from conversations
- ‚úÖ Edit speaker dialogue naturally

## üìã What's Implemented

### ‚úÖ Architecture Complete
- `VoiceCloningAgent` - Handles voice cloning requests
- `SpeechProcessingAgent` - Integrated voice cloning into final audio generation
- Per-speaker reference audio collection
- Automatic fallback to original audio if cloning fails

### ‚ö†Ô∏è VoiceCraft Model - Needs Setup
The model loading is currently a **placeholder**. You need to set up VoiceCraft to enable actual voice cloning.

## üöÄ Quick Start (Without VoiceCraft)

The system works **right now** without VoiceCraft:
- Text modifications will use **original audio** (words are removed/kept)
- Adding words won't be reflected in audio (text-only change)
- Deleting words works perfectly (audio segments removed)

## üîß Setting Up VoiceCraft (Optional but Recommended)

### Option 1: Install VoiceCraft Locally

1. **Clone VoiceCraft repository:**
   ```bash
   cd backend
   git clone https://github.com/jasonppy/VoiceCraft.git
   cd VoiceCraft
   ```

2. **Install dependencies:**
   ```bash
   pip install torch torchaudio
   pip install phonemizer
   pip install encodec
   pip install -r requirements.txt
   ```

3. **Download pre-trained model:**
   ```bash
   # Download from Hugging Face
   # https://huggingface.co/pyp1/VoiceCraft/tree/main
   mkdir -p pretrained_models
   # Download the checkpoint files to this directory
   ```

4. **Update `voice_cloning_agent.py`:**
   ```python
   def _load_voicecraft_model(self):
       if self.model_loaded:
           return

       try:
           import sys
           sys.path.append('./VoiceCraft')  # Add VoiceCraft to path

           from voicecraft.models import voicecraft
           import torch

           # Load model
           model_path = './VoiceCraft/pretrained_models/voicecraft_330M.pth'
           self.voicecraft_model = voicecraft.VoiceCraft.from_pretrained(model_path)
           self.voicecraft_model.eval()

           # Move to GPU if available
           device = 'cuda' if torch.cuda.is_available() else 'cpu'
           self.voicecraft_model = self.voicecraft_model.to(device)

           self.model_loaded = True
           logger.info(f"VoiceCraft model loaded on {device}")
       except Exception as e:
           logger.error(f"Failed to load VoiceCraft: {str(e)}")
           raise
   ```

5. **Implement `_run_voicecraft()` method:**
   ```python
   def _run_voicecraft(self, reference_audio_path: str, original_text: str,
                      modified_text: str, speaker_id: str) -> str:
       import torch
       from voicecraft import inference

       output_path = f"/tmp/modified_{speaker_id}.wav"

       # Run VoiceCraft inference
       with torch.no_grad():
           inference.edit_speech(
               model=self.voicecraft_model,
               reference_audio=reference_audio_path,
               original_transcript=original_text,
               target_transcript=modified_text,
               output_path=output_path
           )

       return output_path
   ```

### Option 2: Use Hugging Face Inference API (Recommended for Heroku)

This doesn't require local model files:

```python
def _load_voicecraft_model(self):
    # No need to load - use API
    self.model_loaded = True
    self.use_api = True

def _run_voicecraft(self, reference_audio_path: str, original_text: str,
                   modified_text: str, speaker_id: str) -> str:
    import requests

    API_URL = "https://api-inference.huggingface.co/models/pyp1/VoiceCraft"
    headers = {"Authorization": f"Bearer {os.environ.get('HF_API_KEY')}"}

    with open(reference_audio_path, "rb") as f:
        audio_data = f.read()

    payload = {
        "inputs": {
            "audio": audio_data,
            "original_text": original_text,
            "target_text": modified_text
        }
    }

    response = requests.post(API_URL, headers=headers, json=payload)

    # Save output
    output_path = f"/tmp/modified_{speaker_id}.wav"
    with open(output_path, "wb") as f:
        f.write(response.content)

    return output_path
```

### Option 3: Use Alternative TTS (Simpler, Good Enough)

Use OpenAI TTS (already in your dependencies):

```python
def _run_voicecraft(self, reference_audio_path: str, original_text: str,
                   modified_text: str, speaker_id: str) -> str:
    from openai import OpenAI
    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))

    # Use OpenAI TTS (won't match voice perfectly, but close)
    response = client.audio.speech.create(
        model="tts-1-hd",
        voice="alloy",  # or "echo", "fable", "onyx", "nova", "shimmer"
        input=modified_text
    )

    output_path = f"/tmp/modified_{speaker_id}.wav"
    response.stream_to_file(output_path)

    return output_path
```

## üí∞ Cost Comparison

| Option | Setup Time | Cost | Quality | Heroku Compatible |
|--------|------------|------|---------|-------------------|
| Local VoiceCraft | 2-3 hours | Free | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå (needs GPU) |
| HF Inference API | 30 mins | $0.01/request | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ |
| OpenAI TTS | 5 mins | $0.015/1K chars | ‚≠ê‚≠ê‚≠ê | ‚úÖ |
| No voice cloning | 0 mins | Free | ‚≠ê‚≠ê (text only) | ‚úÖ |

## üß™ Testing

1. **Restart Flask backend:**
   ```bash
   python run.py
   ```

2. **Upload audio file and transcribe**

3. **Make text modifications:**
   - Delete some words (strike-through)
   - Add new words

4. **Click Submit**

5. **Check Flask console for logs:**
   ```
   [conv_0] Text modified, using voice cloning for speaker A
     Original: 'Hello, how are you doing today?'
     Modified: 'Hi, how are you?'
     Using 3 reference segments from speaker A
     ‚úì Voice cloning successful
   ```

## üìä Current Behavior

### Without VoiceCraft Setup:
- ‚úÖ Deleting words: Works perfectly (audio segments removed)
- ‚ö†Ô∏è Adding words: Text changes, but audio uses original
- ‚ö†Ô∏è Modifying words: Text changes, but audio uses original

### With VoiceCraft Setup:
- ‚úÖ Deleting words: Works perfectly
- ‚úÖ Adding words: New audio generated with speaker's voice
- ‚úÖ Modifying words: New audio generated with speaker's voice

## üéØ Recommended Path

For your **balanced budget approach**, I recommend:

1. **Start with current implementation** (works now, no voice cloning)
2. **Test the workflow** with deletions/keeping original audio
3. **If users need voice cloning**, add **OpenAI TTS** (5-minute setup, good quality)
4. **Later**, upgrade to VoiceCraft if you need perfect voice matching

## üêõ Troubleshooting

### "Voice cloning failed, using original audio"
- VoiceCraft model not loaded
- Check logs for specific error
- Falls back gracefully to original audio

### "Failed to load VoiceCraft model"
- Model files not downloaded
- Dependencies not installed
- Check `backend/mcp_agents/voice_cloning_agent.py` line 20-30

### Slow performance
- Voice cloning takes 5-15 seconds per segment on CPU
- Use GPU or cloud API for faster results

## üìù Next Steps

1. ‚úÖ Current implementation works for basic editing
2. ‚ö†Ô∏è Choose voice cloning option (see above)
3. ‚ö†Ô∏è Update `voice_cloning_agent.py` with chosen method
4. ‚ö†Ô∏è Test with real audio files
5. ‚ö†Ô∏è Deploy to Heroku (if using API-based approach)

## üéâ What You Get Now

Even without full VoiceCraft setup, you have:
- ‚úÖ Async transcription with progress bars
- ‚úÖ Text editing and modification
- ‚úÖ Audio segment removal/reordering
- ‚úÖ Multi-speaker support
- ‚úÖ Architecture ready for voice cloning
- ‚úÖ Automatic speaker grouping and reference collection
- ‚úÖ Graceful fallbacks

**Your app is production-ready for basic editing!** Voice cloning is an optional enhancement.
